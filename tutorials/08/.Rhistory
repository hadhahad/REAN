library(car)
library(lattice)
library(MASS)
library(ggplot2)
library(leaps)
library(ISLR)
library(ISLR)
library(graphics)
library(effects)
install.packages("effects")
setwd("~/Studies/REAN/tutorials/08")
# Shape of dependence - linearity, polynomial regression
summary(lm(dist~speed,data=cars))             # intercept + linear
summary(lm(dist~speed^2 ,data=cars))          # intercept + quadrature
summary(lm(dist~speed+I(speed^2),data=cars))  # intercept + linear + quadrature
summary(lm(dist~poly(speed, degree=3, raw=TRUE),data=cars)) # polynomial regression till d. 3
# Repeating measurement - compare factor model
# Test submodel with factor variable with less DF
with(cars,table(speed))
anova(m1_f<-lm(dist/speed~factor(speed),data=cars))
anova(m1_c<-lm(dist/speed~speed,data=cars))
anova(m1_c,m1_f)
# Use SLID data from car package
? SLID
summary(SLID)
SLID <- na.omit(SLID) # remove NA's
summary(SLID)
mod0 <- lm(wages ~ sex + age + education, data=SLID)
summary(mod0)
# checking normality
op <- par(mfrow=c(1,2))
qq.plot(mod0, simulate=TRUE, line="none")
plot(density(rstudent(mod0)))
par(op)
mod1 <- lm(log(wages) ~ sex + age + education, data=SLID)
summary(mod1)
# compare models with and without log transform
op <- par(mfrow=c(2,2))
qq.plot(mod0, simulate=TRUE, line="none")
plot(density(rstudent(mod0)))
qq.plot(mod1, simulate=TRUE, line="none")
plot(density(rstudent(mod1)))
op <- par(mfrow=c(2,2))
plot(fitted(mod0), rstudent(mod0), col="black",pch=".")
abline(h=0, lty=2)
lines(lowess(fitted(mod0), rstudent(mod0)))
spreadLevelPlot(mod0, col="black",pch=".")
plot(fitted(mod1), rstudent(mod1), col="blue",pch=".")
abline(h=0, lty=2)
lines(lowess(fitted(mod1), rstudent(mod1)))
spreadLevelPlot(mod1,col="blue",pch=".")
par(op)
# checking linearity
crPlots(mod1)
mod2 <- lm(log(wages) ~ sex + poly(age, degree=2, raw=TRUE) + I(education^2), data=SLID)
mod2 <- lm(log(wages) ~ sex + age + I(age^2) + I(education^2),    data=SLID) # same
summary(mod2)
crPlots(mod2)
plot(allEffects(mod1))
library(effects)
plot(allEffects(mod1))
plot(allEffects(mod2))
plot(allEffects(mod1))
plot(allEffects(mod2))
boxcox(mod0)
dev.off()
boxcox(mod0)
mod.bc <- update(mod0, . ~ . + boxCoxVariable(wages))
mod_bc <- update(mod0, . ~ . + boxCoxVariable(wages))
summary(mod_bc)
avPlots(mod_bc)  # constructed-variable plot
# Box-Tidwell regression
boxTidwell(log(wages) ~ I(age - 15) + I(education + 1), other.x=~sex, data=SLID)
mod_bt <- lm(log(wages) ~ I(age - 15) + I(education + 1) + sex
+ I((age - 15)*log(age - 15)) + I((education + 1)*log(education + 1)),
data=SLID)
avPlots(mod_bt)  # constructed-variable plots
summary(mod_bt)
# Exponential distribution
mod_g1 <- glm(wages ~ sex + age + I(age^2) + I(education^2), family=Gamma(link = "inverse"),   data=SLID) # same
summary(mod_g1)
residP_mode_g1 <- residuals(mod_g1, type = "pearson")
residD_mode_g1 <- residuals(mod_g1, type = "devi")
plot(residP_mode_g1,residD_mode_g1)
opar<-par(mfrow = c(2,2))
plot(mod_g1)
par(opar)
#The logit of a probability is simply the log of the odds of the response taking the value one.
#generate Bernoulli probabilities from true model
x <-rnorm(100)
p<-exp(x)/(1+exp(x))
#one replication per predictor value
n <- rep(1,100)
#simulate response
y1 <- rbinom(100,n,p)
#fit model
mod_g21 <- glm(y1~x,family=binomial(link = "logit"))
#make quantile-quantile plot of residuals
qqnorm(residuals(mod_g21, type="deviance"))
abline(a=0,b=1)
n <- rep(30,100)
#simulate response
y2<-rbinom(100,n,p)
#fit model
mod_g22 <- glm(cbind(y2,n-y2)~x,family="binomial")
#make quantile-quantile plot of residuals
qqnorm(residuals(mod_g22, type="deviance"))
abline(a=0,b=1)
# Get Boston data from car package
head(Boston)
? Boston
summary(Boston)
model1 <- lm(medv ~ lstat, data = Boston)
plot(model1)
model1 <- lm(medv ~ (lstat)^2, data = Boston)
plot(model1)
# 2) Try to predict response chas as a function of other variables (crim, indus, medv, rad)
# by GLM and logistic regression
dev.off()
model1 <- lm(medv ~ (lstat)^2, data = Boston)
plot(model1)
par <-op(mfrow(c(2,2)))
opar<-par(mfrow = c(2,2))
model1 <- lm(medv ~ (lstat)^2, data = Boston)
plot(model1)
?poly
model1 <- lm(medv ~ I(lstat^2), data = Boston)
plot(model1)
model1 <- lm(medv ~ poly(lstat, degree=3, raw = True), data = Boston)
plot(model1)
model1 <- lm(medv ~ poly(lstat, degree=3, raw=T), data = Boston)
plot(model1)
par(opar)
model2 <- lm(medv ~ log(lstat), data = Boston)
plot(model2)
# by GLM and logistic regression
dev.off()
opar<-par(mfrow = c(2,2))
model2 <- lm(medv ~ log(lstat), data = Boston)
plot(model2)
par(op)
par(opar)
par(op)
par(op)
par(op)
model3 <- lm(log(medv) ~ lstat, data = Boston)
plot(model3)
op <- par(mfrow=c(2,2))
plot(fitted(mod0), rstudent(mod0), col="black",pch=".")
abline(h=0, lty=2)
lines(lowess(fitted(mod0), rstudent(mod0)))
spreadLevelPlot(mod0, col="black",pch=".")
plot(fitted(mod1), rstudent(mod1), col="blue",pch=".")
abline(h=0, lty=2)
lines(lowess(fitted(mod1), rstudent(mod1)))
spreadLevelPlot(mod1,col="blue",pch=".")
par(op)
# checking linearity
crPlots(mod1)
crPlots(model3)
model3 <- lm(log(medv) ~ lstat^2, data = Boston)
plot(model3)
crPlots(model3)
opar<-par(mfrow = c(2,2))
model3 <- lm(log(medv) ~ lstat^2, data = Boston)
crPlots(model3)
model3 <- lm(log(medv) ~ I(lstat^2), data = Boston)
plot(model3)
crPlots(model3)
model3 <- lm(log(medv) ~ poly(lstat, degree=2, raw=TRUE), data = Boston)
crPlots(model3)
